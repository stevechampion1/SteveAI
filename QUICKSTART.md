# SteveAI - å¿«é€Ÿå¼€å§‹æŒ‡å—

## æ¦‚è¿°

SteveAI æ˜¯ä¸€ä¸ªåŸºäºçŸ¥è¯†è’¸é¦çš„å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼Œå¯ä»¥å¸®åŠ©æ‚¨è®­ç»ƒæ›´å°ã€æ›´å¿«çš„å­¦ç”Ÿæ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒæ¥è¿‘æ•™å¸ˆæ¨¡å‹çš„æ€§èƒ½ã€‚

## ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- **GPU**: NVIDIA GPU with CUDA support (æ¨è RTX 3080 æˆ–æ›´é«˜)
- **å†…å­˜**: è‡³å°‘ 16GB RAM
- **å­˜å‚¨**: è‡³å°‘ 50GB å¯ç”¨ç©ºé—´

### è½¯ä»¶è¦æ±‚
- Python 3.8+
- CUDA 11.0+ (å¦‚æœä½¿ç”¨GPU)
- PyTorch 1.12+

## å®‰è£…

### 1. å…‹éš†é¡¹ç›®

```bash
git clone https://github.com/your-username/SteveAI.git
cd SteveAI
```

### 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
# ä½¿ç”¨ conda (æ¨è)
conda create -n steveai python=3.9
conda activate steveai

# æˆ–ä½¿ç”¨ venv
python -m venv steveai_env
source steveai_env/bin/activate  # Linux/Mac
# steveai_env\Scripts\activate  # Windows
```

### 3. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 4. éªŒè¯å®‰è£…

```bash
python -c "import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')"
```

## å¿«é€Ÿå¼€å§‹

### æ­¥éª¤ 1: æ•™å¸ˆæ¨¡å‹æ¨ç†

é¦–å…ˆè¿è¡Œæ•™å¸ˆæ¨¡å‹æ¨ç†ï¼Œç”Ÿæˆç”¨äºçŸ¥è¯†è’¸é¦çš„logitsï¼š

```bash
python teacher_inference_gpu.py
```

è¿™å°†ï¼š
- åŠ è½½ DeepSeek-Coder-6.7B æ•™å¸ˆæ¨¡å‹
- åœ¨ Alpaca æ•°æ®é›†ä¸Šè¿è¡Œæ¨ç†
- ä¿å­˜æ•™å¸ˆlogitsåˆ° `./output/SteveAI_Teacher_Inference/teacher_logits_float16/`

### æ­¥éª¤ 2: å­¦ç”Ÿæ¨¡å‹è®­ç»ƒ

ä½¿ç”¨æ•™å¸ˆlogitsè®­ç»ƒå­¦ç”Ÿæ¨¡å‹ï¼š

```bash
python student_training.py \
    --teacher_logits_path ./output/SteveAI_Teacher_Inference/teacher_logits_float16 \
    --output_dir ./output/student_model \
    --num_epochs 3 \
    --batch_size 8
```

### æ­¥éª¤ 3: æ¨¡å‹è¯„ä¼°

è¯„ä¼°è®­ç»ƒå¥½çš„å­¦ç”Ÿæ¨¡å‹ï¼š

```bash
python evaluate.py \
    --model_path ./output/student_model/final_model \
    --tokenizer_path ./output/student_model/final_model \
    --teacher_logits_path ./output/SteveAI_Teacher_Inference/teacher_logits_float16 \
    --output_dir ./output/evaluation_results
```

### æ­¥éª¤ 4: æ€§èƒ½åŸºå‡†æµ‹è¯•

è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼š

```bash
python benchmark.py \
    --model_path ./output/student_model/final_model \
    --tokenizer_path ./output/student_model/final_model \
    --teacher_logits_path ./output/SteveAI_Teacher_Inference/teacher_logits_float16 \
    --output_dir ./output/benchmark_results
```

## é…ç½®è¯´æ˜

### åŸºæœ¬é…ç½® (config.yaml)

```yaml
# æ¨¡å‹é…ç½®
model:
  teacher_model_id: "deepseek-ai/deepseek-coder-6.7b-instruct"
  student_model_id: "deepseek-ai/deepseek-coder-1.3b-instruct"

# è®­ç»ƒé…ç½®
training:
  learning_rate: 5e-5
  num_epochs: 3
  batch_size: 8
  temperature: 4.0
  alpha: 0.7  # è’¸é¦æŸå¤±æƒé‡
  beta: 0.3   # ç¡¬æŸå¤±æƒé‡

# æ•°æ®é›†é…ç½®
dataset:
  dataset_id: "yahma/alpaca-cleaned"
  dataset_subset_size: 500
  max_seq_length: 256
```

### ç¯å¢ƒé…ç½®

é¡¹ç›®ä¼šè‡ªåŠ¨æ£€æµ‹è¿è¡Œç¯å¢ƒï¼š
- **Kaggle**: è‡ªåŠ¨ä½¿ç”¨ `/kaggle/working/` è·¯å¾„
- **æœ¬åœ°**: ä½¿ç”¨ `./output/` è·¯å¾„

## ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨

```python
from config_manager import ConfigManager
from data_utils import prepare_student_dataset
from distillation_loss import DistillationLoss
from student_training import load_student_model_and_tokenizer

# åŠ è½½é…ç½®
config = ConfigManager("config.yaml")

# åŠ è½½æ¨¡å‹
model, tokenizer = load_student_model_and_tokenizer()

# å‡†å¤‡æ•°æ®
dataset = prepare_student_dataset(
    teacher_logits_path="./output/teacher_logits",
    tokenizer=tokenizer
)

# åˆ›å»ºæŸå¤±å‡½æ•°
loss_fn = DistillationLoss()

# è®­ç»ƒæ¨¡å‹
# ... è®­ç»ƒä»£ç 
```

### é«˜çº§ä½¿ç”¨

```python
from distillation_loss import AdvancedDistillationLoss
from optimize_model import ModelOptimizer
from deploy import ModelServer

# ä½¿ç”¨é«˜çº§è’¸é¦æŸå¤±
loss_fn = AdvancedDistillationLoss()

# æ¨¡å‹ä¼˜åŒ–
optimizer = ModelOptimizer(model)
quantized_model = optimizer.quantize_model('dynamic')

# æ¨¡å‹éƒ¨ç½²
server = ModelServer(model_path="./model", tokenizer_path="./model")
server.load_model()
```

## å¸¸è§é—®é¢˜

### Q: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

A: å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š
1. å‡å° `batch_size`
2. å‡å° `max_seq_length`
3. ä½¿ç”¨ `gradient_accumulation_steps`
4. å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

### Q: è®­ç»ƒé€Ÿåº¦æ…¢æ€ä¹ˆåŠï¼Ÿ

A: ä¼˜åŒ–å»ºè®®ï¼š
1. å¢åŠ  `num_workers` ç”¨äºæ•°æ®åŠ è½½
2. ä½¿ç”¨æ›´å¤§çš„ `batch_size`
3. å¯ç”¨ `pin_memory=True`
4. ä½¿ç”¨æ›´å¿«çš„å­˜å‚¨è®¾å¤‡

### Q: å¦‚ä½•è°ƒæ•´è’¸é¦å‚æ•°ï¼Ÿ

A: å…³é”®å‚æ•°ï¼š
- `temperature`: æ§åˆ¶è½¯ç›®æ ‡å¹³æ»‘åº¦ (æ¨è 3-5)
- `alpha`: è’¸é¦æŸå¤±æƒé‡ (æ¨è 0.7)
- `beta`: ç¡¬æŸå¤±æƒé‡ (æ¨è 0.3)

### Q: å¦‚ä½•ç›‘æ§è®­ç»ƒè¿‡ç¨‹ï¼Ÿ

A: ä½¿ç”¨è®­ç»ƒç›‘æ§ï¼š

```bash
python monitor_training.py --log_dir ./logs
```

## æ€§èƒ½ä¼˜åŒ–

### 1. å†…å­˜ä¼˜åŒ–

```python
# å®šæœŸæ¸…ç†GPUç¼“å­˜
import torch
torch.cuda.empty_cache()

# ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
config.set('training.gradient_accumulation_steps', 4)
```

### 2. é€Ÿåº¦ä¼˜åŒ–

```python
# ä½¿ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½
config.set('inference.num_workers', 4)

# å¯ç”¨æ··åˆç²¾åº¦
config.set('training.use_amp', True)
```

### 3. å­˜å‚¨ä¼˜åŒ–

```python
# ä½¿ç”¨å‹ç¼©ä¿å­˜
torch.save(model.state_dict(), 'model.pt', _use_new_zipfile_serialization=True)
```

## ç»“æœåˆ†æ

### è®­ç»ƒç»“æœ

è®­ç»ƒå®Œæˆåï¼Œæ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®æ‰¾åˆ°ç»“æœï¼š

- **æ¨¡å‹æ£€æŸ¥ç‚¹**: `./output/checkpoints/`
- **æœ€ç»ˆæ¨¡å‹**: `./output/final_model/`
- **è¯„ä¼°ç»“æœ**: `./output/evaluation_results/`
- **åŸºå‡†æµ‹è¯•**: `./output/benchmark_results/`

### å¯è§†åŒ–ç»“æœ

```bash
python visualize_results.py \
    --results_dir ./output \
    --output_dir ./visualizations
```

## ä¸‹ä¸€æ­¥

1. **é˜…è¯»å®Œæ•´æ–‡æ¡£**: æŸ¥çœ‹ [API_DOCUMENTATION.md](API_DOCUMENTATION.md)
2. **è¿è¡Œç¤ºä¾‹**: æŸ¥çœ‹ [examples/](examples/) ç›®å½•
3. **è‡ªå®šä¹‰é…ç½®**: ä¿®æ”¹ `config.yaml` é€‚åº”æ‚¨çš„éœ€æ±‚
4. **æ‰©å±•åŠŸèƒ½**: åŸºäºç°æœ‰ä»£ç æ·»åŠ æ–°åŠŸèƒ½

## è·å–å¸®åŠ©

- **GitHub Issues**: æŠ¥å‘Šbugæˆ–è¯·æ±‚åŠŸèƒ½
- **æ–‡æ¡£**: æŸ¥çœ‹å®Œæ•´çš„APIæ–‡æ¡£
- **ç¤ºä¾‹**: è¿è¡Œæä¾›çš„ç¤ºä¾‹ä»£ç 

## è®¸å¯è¯

æœ¬é¡¹ç›®ä½¿ç”¨ Apache License 2.0 è®¸å¯è¯ã€‚è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

---

**å¼€å§‹æ‚¨çš„çŸ¥è¯†è’¸é¦ä¹‹æ—…å§ï¼** ğŸš€
